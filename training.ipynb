{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import dgl\n",
    "import pickle\n",
    "from utils import HGT\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.load_graphs('training_data/graph.dgl')\n",
    "graph = graph[0][0]\n",
    "\n",
    "with open('training_data/train.obj', 'rb') as fp:\n",
    "\ttrain = pickle.load(fp)\n",
    "\n",
    "with open('training_data/val.obj', 'rb') as fp:\n",
    "\tval = pickle.load(fp)\n",
    "\n",
    "with open('training_data/test.obj', 'rb') as fp:\n",
    "\ttest = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150243"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes('business').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r3/89932hws5rg2xwsqck66y26r0000gp/T/ipykernel_13761/4158148776.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g.ndata['feat'] = {k: torch.tensor(v, dtype=torch.float32) for k, v in graph.ndata['feat'].items() }\n"
     ]
    }
   ],
   "source": [
    "edges = {}\n",
    "for canonical_etype in graph.canonical_etypes:\n",
    "    edges[canonical_etype] = graph.edges(etype=canonical_etype)\n",
    "\n",
    "edges[('category', 'category_to_business', 'business')] = (graph.edges(etype='business_has_category')[1], graph.edges(etype='business_has_category')[0])\n",
    "edges[('business', 'business_to_review', 'review')] = (graph.edges(etype='review_to_business')[1], graph.edges(etype='review_to_business')[0])\n",
    "edges[('business', 'business_to_tip', 'tip')] = (graph.edges(etype='tip_to_business')[1], graph.edges(etype='tip_to_business')[0])\n",
    "edges[('review', 'review_to_user', 'user')] = (graph.edges(etype='user_to_review')[1], graph.edges(etype='user_to_review')[0])\n",
    "edges[('tip', 'tip_to_user', 'user')] = (graph.edges(etype='user_to_tip')[1], graph.edges(etype='user_to_tip')[0])\n",
    "\n",
    "num_nodes_dict = {} \n",
    "for ntype in graph.ntypes:\n",
    "    num_nodes_dict[ntype] = graph.nodes(ntype).shape[0]\n",
    "\n",
    "g = dgl.heterograph(edges, num_nodes_dict = num_nodes_dict)\n",
    "g.ndata['feat'] = {k: torch.tensor(v, dtype=torch.float32) for k, v in graph.ndata['feat'].items() }\n",
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DGLGraph.num_nodes of Graph(num_nodes={'business': 150243, 'category': 1311, 'review': 6339837, 'tip': 908878, 'user': 1987897},\n",
       "      num_edges={('business', 'business_has_category', 'category'): 668592, ('business', 'business_to_review', 'review'): 6339837, ('business', 'business_to_tip', 'tip'): 908878, ('category', 'category_to_business', 'business'): 668592, ('review', 'review_to_business', 'business'): 6339837, ('review', 'review_to_user', 'user'): 6339837, ('tip', 'tip_to_business', 'business'): 908878, ('tip', 'tip_to_user', 'user'): 908878, ('user', 'user_to_review', 'review'): 6339837, ('user', 'user_to_tip', 'tip'): 908878, ('user', 'user_to_user', 'user'): 437928},\n",
       "      metagraph=[('business', 'category', 'business_has_category'), ('business', 'review', 'business_to_review'), ('business', 'tip', 'business_to_tip'), ('category', 'business', 'category_to_business'), ('review', 'business', 'review_to_business'), ('review', 'user', 'review_to_user'), ('tip', 'business', 'tip_to_business'), ('tip', 'user', 'tip_to_user'), ('user', 'review', 'user_to_review'), ('user', 'tip', 'user_to_tip'), ('user', 'user', 'user_to_user')])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = { ntype: g.ntypes.index(ntype) for ntype in g.ntypes }\n",
    "edge_dict = { canonical_etype: g.canonical_etypes.index(canonical_etype) for canonical_etype in g.canonical_etypes }\n",
    "feature_dim_dict = { ntype: g.ndata['feat'][ntype].shape[1] for ntype in g.ntypes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 0, 'category': 1, 'review': 2, 'tip': 3, 'user': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HGT(node_dict, edge_dict, feature_dim_dict, n_hid=256, n_out=128, n_layers=4, n_heads=8, use_norm=False)\n",
    "opt = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "sampler = dgl.dataloading.NeighborSampler([24, 24, 24, 24])\n",
    "criterion = torch.nn.MarginRankingLoss(margin=0.1)\n",
    "# dgl.dataloading.NeighborSampler([\n",
    "#     {('user', 'follows', 'user'): 5,\n",
    "#      ('user', 'plays', 'game'): 4,\n",
    "#      ('game', 'played-by', 'user'): 3}] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_ids = list(range(train['pos'][0].shape[0]))\n",
    "train_neg_ids = list(range(train['neg'][0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135108"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_neg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['feat']['category'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(g, model, pos_ids, neg_ids, relation_tuple, sampler, batch_size):\n",
    "    pos_users = torch.index_select(relation_tuple['pos'][0], 0, torch.tensor(pos_ids))\n",
    "    pos_users_unique, pos_users_inverse = torch.unique(pos_users, return_inverse=True)\n",
    "    pos_block_user = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'user': pos_users_unique}, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "    pos_business = torch.index_select(relation_tuple['pos'][1], 0, torch.tensor(pos_ids))\n",
    "    pos_business_unique, pos_business_inverse = torch.unique(pos_business, return_inverse=True)\n",
    "    pos_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'business': pos_business_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "    \n",
    "    neg_users = torch.index_select(relation_tuple['neg'][0], 0, torch.tensor(neg_ids))\n",
    "    neg_users_unique, neg_users_inverse = torch.unique(neg_users, return_inverse=True)\n",
    "    neg_block_user = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'user': neg_users_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "    \n",
    "    neg_business = torch.index_select(relation_tuple['neg'][1], 0, torch.tensor(neg_ids))\n",
    "    neg_business_unique, neg_business_inverse = torch.unique(neg_business, return_inverse=True)\n",
    "    neg_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'business': neg_business_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "    pos_user_logits = torch.index_select(model(pos_block_user, 'user'), 0, pos_users_inverse)\n",
    "    pos_business_logits = torch.index_select(model(pos_block_business, 'business'), 0, pos_business_inverse)\n",
    "    neg_user_logits = torch.index_select(model(neg_block_user, 'user'), 0, neg_users_inverse)\n",
    "    neg_business_logits = torch.index_select(model(neg_block_business, 'business'), 0, neg_business_inverse)\n",
    "    return pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0011257501319050789\n",
      "0.0009124502539634705\n",
      "0.0009070197120308876\n",
      "0.0006720079109072685\n",
      "0.0006000311113893986\n",
      "0.0002428349107503891\n",
      "0.00031552277505397797\n",
      "0.00036688148975372314\n",
      "0.0003705797716975212\n",
      "0.0003163143992424011\n",
      "0.0003445069305598736\n",
      "0.000242678914219141\n",
      "0.0003475630655884743\n",
      "0.00017621507868170738\n",
      "0.0003565829247236252\n",
      "0.00036633387207984924\n",
      "0.0003993348218500614\n",
      "0.00033133476972579956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m opt\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m pos_ids, neg_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))\n\u001b[0;32m---> 13\u001b[0m pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits \u001b[39m=\u001b[39m predict(g, model, pos_ids, neg_ids, train, sampler, batch_size)\n\u001b[1;32m     14\u001b[0m pos_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(pos_user_logits\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mn_out), pos_business_logits\u001b[39m.\u001b[39mview(batch_size, model\u001b[39m.\u001b[39mn_out, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     15\u001b[0m neg_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(neg_user_logits\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mn_out), neg_business_logits\u001b[39m.\u001b[39mview(batch_size, model\u001b[39m.\u001b[39mn_out, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[28], line 29\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(g, model, pos_ids, neg_ids, relation_tuple, sampler, batch_size)\u001b[0m\n\u001b[1;32m     27\u001b[0m pos_business_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(pos_block_business, \u001b[39m'\u001b[39m\u001b[39mbusiness\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m, pos_business_inverse)\n\u001b[1;32m     28\u001b[0m neg_user_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(neg_block_user, \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m, neg_users_inverse)\n\u001b[0;32m---> 29\u001b[0m neg_business_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(neg_block_business, \u001b[39m'\u001b[39;49m\u001b[39mbusiness\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m0\u001b[39m, neg_business_inverse)\n\u001b[1;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/workspace/yelp-dataset/utils/hgt.py:145\u001b[0m, in \u001b[0;36mHGT.forward\u001b[0;34m(self, G, out_key)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mfor\u001b[39;00m ntype \u001b[39min\u001b[39;00m G[i]\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    144\u001b[0m         h[\u001b[39m'\u001b[39m\u001b[39mdst\u001b[39m\u001b[39m'\u001b[39m][ntype] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_norm(F\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapt_ws[ntype](G[i]\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m][ntype])))\n\u001b[0;32m--> 145\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcs[i](G[i], h)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out_key, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    147\u001b[0m     h[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m][out_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(h[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m][out_key])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/workspace/yelp-dataset/utils/hgt.py:109\u001b[0m, in \u001b[0;36mHGTLayer.forward\u001b[0;34m(self, G, h)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[39mif\u001b[39;00m G\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mget(ntype) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m     t \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mgelu(G\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m][ntype])\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_dim)\n\u001b[0;32m--> 109\u001b[0m     trans_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdrop(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49ma_linears[n_id](t))\n\u001b[1;32m    110\u001b[0m     trans_out \u001b[39m=\u001b[39m trans_out \u001b[39m*\u001b[39m alpha \u001b[39m+\u001b[39m h[\u001b[39m'\u001b[39m\u001b[39mdst\u001b[39m\u001b[39m'\u001b[39m][ntype] \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m-\u001b[39malpha)\n\u001b[1;32m    111\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_norm:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/dropout.py:59\u001b[0m, in \u001b[0;36mDropout.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 59\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mp, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minplace)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/functional.py:1252\u001b[0m, in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \u001b[39mif\u001b[39;00m p \u001b[39m<\u001b[39m \u001b[39m0.0\u001b[39m \u001b[39mor\u001b[39;00m p \u001b[39m>\u001b[39m \u001b[39m1.0\u001b[39m:\n\u001b[1;32m   1251\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mdropout probability has to be between 0 and 1, \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(p))\n\u001b[0;32m-> 1252\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39mdropout_(\u001b[39minput\u001b[39m, p, training) \u001b[39mif\u001b[39;00m inplace \u001b[39melse\u001b[39;00m _VF\u001b[39m.\u001b[39;49mdropout(\u001b[39minput\u001b[39;49m, p, training)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split(list_a, chunk_size):\n",
    "    for i in range(0, len(list_a), chunk_size):\n",
    "        yield list_a[i:i + chunk_size]\n",
    "\n",
    "batch_size = 64\n",
    "for epoch in range(1):\n",
    "    model.train()\n",
    "    random.shuffle(train_pos_ids)\n",
    "    random.shuffle(train_neg_ids)\n",
    "    for batch in split(list(zip(train_pos_ids, train_neg_ids)), batch_size):\n",
    "        opt.zero_grad()\n",
    "        pos_ids, neg_ids = list(zip(*batch))\n",
    "        pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits = predict(g, model, pos_ids, neg_ids, train, sampler, batch_size)\n",
    "        pos_score = torch.bmm(pos_user_logits.view(batch_size, 1, model.n_out), pos_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        neg_score = torch.bmm(neg_user_logits.view(batch_size, 1, model.n_out), neg_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        loss = criterion(pos_score, neg_score, torch.ones(batch_size))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        print(loss.item())\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
