{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import dgl\n",
    "import pickle\n",
    "from utils import HGT\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = dgl.load_graphs('training_data/graph.dgl')\n",
    "graph = graph[0][0]\n",
    "\n",
    "with open('training_data/train.obj', 'rb') as fp:\n",
    "\ttrain = pickle.load(fp)\n",
    "\n",
    "with open('training_data/val.obj', 'rb') as fp:\n",
    "\tval = pickle.load(fp)\n",
    "\n",
    "with open('training_data/test.obj', 'rb') as fp:\n",
    "\ttest = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150243"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.nodes('business').shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/r3/89932hws5rg2xwsqck66y26r0000gp/T/ipykernel_2214/4158148776.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  g.ndata['feat'] = {k: torch.tensor(v, dtype=torch.float32) for k, v in graph.ndata['feat'].items() }\n"
     ]
    }
   ],
   "source": [
    "edges = {}\n",
    "for canonical_etype in graph.canonical_etypes:\n",
    "    edges[canonical_etype] = graph.edges(etype=canonical_etype)\n",
    "\n",
    "edges[('category', 'category_to_business', 'business')] = (graph.edges(etype='business_has_category')[1], graph.edges(etype='business_has_category')[0])\n",
    "edges[('business', 'business_to_review', 'review')] = (graph.edges(etype='review_to_business')[1], graph.edges(etype='review_to_business')[0])\n",
    "edges[('business', 'business_to_tip', 'tip')] = (graph.edges(etype='tip_to_business')[1], graph.edges(etype='tip_to_business')[0])\n",
    "edges[('review', 'review_to_user', 'user')] = (graph.edges(etype='user_to_review')[1], graph.edges(etype='user_to_review')[0])\n",
    "edges[('tip', 'tip_to_user', 'user')] = (graph.edges(etype='user_to_tip')[1], graph.edges(etype='user_to_tip')[0])\n",
    "\n",
    "num_nodes_dict = {} \n",
    "for ntype in graph.ntypes:\n",
    "    num_nodes_dict[ntype] = graph.nodes(ntype).shape[0]\n",
    "\n",
    "g = dgl.heterograph(edges, num_nodes_dict = num_nodes_dict)\n",
    "g.ndata['feat'] = {k: torch.tensor(v, dtype=torch.float32) for k, v in graph.ndata['feat'].items() }\n",
    "del graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DGLGraph.num_nodes of Graph(num_nodes={'business': 150243, 'category': 1311, 'review': 6339837, 'tip': 908878, 'user': 1987897},\n",
       "      num_edges={('business', 'business_has_category', 'category'): 668592, ('business', 'business_to_review', 'review'): 6339837, ('business', 'business_to_tip', 'tip'): 908878, ('category', 'category_to_business', 'business'): 668592, ('review', 'review_to_business', 'business'): 6339837, ('review', 'review_to_user', 'user'): 6339837, ('tip', 'tip_to_business', 'business'): 908878, ('tip', 'tip_to_user', 'user'): 908878, ('user', 'user_to_review', 'review'): 6339837, ('user', 'user_to_tip', 'tip'): 908878, ('user', 'user_to_user', 'user'): 437928},\n",
       "      metagraph=[('business', 'category', 'business_has_category'), ('business', 'review', 'business_to_review'), ('business', 'tip', 'business_to_tip'), ('category', 'business', 'category_to_business'), ('review', 'business', 'review_to_business'), ('review', 'user', 'review_to_user'), ('tip', 'business', 'tip_to_business'), ('tip', 'user', 'tip_to_user'), ('user', 'review', 'user_to_review'), ('user', 'tip', 'user_to_tip'), ('user', 'user', 'user_to_user')])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_dict = { ntype: g.ntypes.index(ntype) for ntype in g.ntypes }\n",
    "edge_dict = { canonical_etype: g.canonical_etypes.index(canonical_etype) for canonical_etype in g.canonical_etypes }\n",
    "feature_dim_dict = { ntype: g.ndata['feat'][ntype].shape[1] for ntype in g.ntypes }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'business': 0, 'category': 1, 'review': 2, 'tip': 3, 'user': 4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HGT(node_dict, edge_dict, feature_dim_dict, n_hid=256, n_out=128, n_layers=4, n_heads=8, use_norm=False)\n",
    "opt = torch.optim.AdamW(model.parameters(), 1e-4)\n",
    "sampler = dgl.dataloading.NeighborSampler([24, 24, 24, 24])\n",
    "criterion = torch.nn.MarginRankingLoss(margin=1)\n",
    "# dgl.dataloading.NeighborSampler([\n",
    "#     {('user', 'follows', 'user'): 5,\n",
    "#      ('user', 'plays', 'game'): 4,\n",
    "#      ('game', 'played-by', 'user'): 3}] * 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_ids = list(range(train['pos'][0].shape[0]))\n",
    "train_neg_ids = list(range(train['neg'][0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pos_ids = list(range(val['pos'][0].shape[0]))\n",
    "val_neg_ids = list(range(val['neg'][0].shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_neg_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.ndata['feat']['category'].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(g, model, pos_ids, neg_ids, relation_tuple, sampler, batch_size):\n",
    "    pos_users = torch.index_select(relation_tuple['pos'][0], 0, torch.tensor(pos_ids))\n",
    "    pos_users_unique, pos_users_inverse = torch.unique(pos_users, return_inverse=True)\n",
    "    pos_block_user = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'user': pos_users_unique}, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "    pos_business = torch.index_select(relation_tuple['pos'][1], 0, torch.tensor(pos_ids))\n",
    "    pos_business_unique, pos_business_inverse = torch.unique(pos_business, return_inverse=True)\n",
    "    pos_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'business': pos_business_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "    \n",
    "    neg_users = torch.index_select(relation_tuple['neg'][0], 0, torch.tensor(neg_ids))\n",
    "    neg_users_unique, neg_users_inverse = torch.unique(neg_users, return_inverse=True)\n",
    "    neg_block_user = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'user': neg_users_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "    \n",
    "    neg_business = torch.index_select(relation_tuple['neg'][1], 0, torch.tensor(neg_ids))\n",
    "    neg_business_unique, neg_business_inverse = torch.unique(neg_business, return_inverse=True)\n",
    "    neg_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "        g, {'business': neg_business_unique }, sampler,\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "    pos_user_logits = torch.index_select(model(pos_block_user, 'user'), 0, pos_users_inverse)\n",
    "    pos_business_logits = torch.index_select(model(pos_block_business, 'business'), 0, pos_business_inverse)\n",
    "    neg_user_logits = torch.index_select(model(neg_block_user, 'user'), 0, neg_users_inverse)\n",
    "    neg_business_logits = torch.index_select(model(neg_block_business, 'business'), 0, neg_business_inverse)\n",
    "    return pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33777"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(zip(val_pos_ids, val_neg_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m split(\u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(val_pos_ids, val_neg_ids)), batch_size):\n\u001b[1;32m     33\u001b[0m     pos_ids, neg_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))\n\u001b[0;32m---> 34\u001b[0m     pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits \u001b[39m=\u001b[39m predict(g, model, pos_ids, neg_ids, val, sampler, batch_size)\n\u001b[1;32m     35\u001b[0m     pos_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(pos_user_logits\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mn_out), pos_business_logits\u001b[39m.\u001b[39mview(batch_size, model\u001b[39m.\u001b[39mn_out, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n\u001b[1;32m     36\u001b[0m     neg_score \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbmm(neg_user_logits\u001b[39m.\u001b[39mview(batch_size, \u001b[39m1\u001b[39m, model\u001b[39m.\u001b[39mn_out), neg_business_logits\u001b[39m.\u001b[39mview(batch_size, model\u001b[39m.\u001b[39mn_out, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39msqueeze()\n",
      "Cell \u001b[0;32mIn[14], line 26\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(g, model, pos_ids, neg_ids, relation_tuple, sampler, batch_size)\u001b[0m\n\u001b[1;32m     21\u001b[0m neg_business_unique, neg_business_inverse \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39munique(neg_business, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     22\u001b[0m neg_block_business \u001b[39m=\u001b[39m [blocks \u001b[39mfor\u001b[39;00m _, _, blocks \u001b[39min\u001b[39;00m dgl\u001b[39m.\u001b[39mdataloading\u001b[39m.\u001b[39mDataLoader(\n\u001b[1;32m     23\u001b[0m     g, {\u001b[39m'\u001b[39m\u001b[39mbusiness\u001b[39m\u001b[39m'\u001b[39m: neg_business_unique }, sampler,\n\u001b[1;32m     24\u001b[0m     batch_size\u001b[39m=\u001b[39mbatch_size, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, drop_last\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)][\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 26\u001b[0m pos_user_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(pos_block_user, \u001b[39m'\u001b[39;49m\u001b[39muser\u001b[39;49m\u001b[39m'\u001b[39;49m), \u001b[39m0\u001b[39m, pos_users_inverse)\n\u001b[1;32m     27\u001b[0m pos_business_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(pos_block_business, \u001b[39m'\u001b[39m\u001b[39mbusiness\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m, pos_business_inverse)\n\u001b[1;32m     28\u001b[0m neg_user_logits \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mindex_select(model(neg_block_user, \u001b[39m'\u001b[39m\u001b[39muser\u001b[39m\u001b[39m'\u001b[39m), \u001b[39m0\u001b[39m, neg_users_inverse)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/workspace/yelp-dataset/utils/hgt.py:145\u001b[0m, in \u001b[0;36mHGT.forward\u001b[0;34m(self, G, out_key)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[39mfor\u001b[39;00m ntype \u001b[39min\u001b[39;00m G[i]\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    144\u001b[0m         h[\u001b[39m'\u001b[39m\u001b[39mdst\u001b[39m\u001b[39m'\u001b[39m][ntype] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_norm(F\u001b[39m.\u001b[39mgelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madapt_ws[ntype](G[i]\u001b[39m.\u001b[39mdstdata[\u001b[39m'\u001b[39m\u001b[39mfeat\u001b[39m\u001b[39m'\u001b[39m][ntype])))\n\u001b[0;32m--> 145\u001b[0m     h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgcs[i](G[i], h)\n\u001b[1;32m    146\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(out_key, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    147\u001b[0m     h[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m][out_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mnormalize(h[\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m][out_key])\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Desktop/workspace/yelp-dataset/utils/hgt.py:95\u001b[0m, in \u001b[0;36mHGTLayer.forward\u001b[0;34m(self, G, h)\u001b[0m\n\u001b[1;32m     92\u001b[0m     attn_score \u001b[39m=\u001b[39m edge_softmax(sub_graph, attn_score, norm_by\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdst\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m     sub_graph\u001b[39m.\u001b[39medata[\u001b[39m'\u001b[39m\u001b[39mt\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m attn_score\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m G\u001b[39m.\u001b[39;49mmulti_update_all({etype : (fn\u001b[39m.\u001b[39;49mu_mul_e(\u001b[39m'\u001b[39;49m\u001b[39mv_\u001b[39;49m\u001b[39m%d\u001b[39;49;00m\u001b[39m'\u001b[39;49m \u001b[39m%\u001b[39;49m e_id, \u001b[39m'\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m'\u001b[39;49m), fn\u001b[39m.\u001b[39;49msum(\u001b[39m'\u001b[39;49m\u001b[39mm\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mt\u001b[39;49m\u001b[39m'\u001b[39;49m)) \\\n\u001b[1;32m     96\u001b[0m                     \u001b[39mfor\u001b[39;49;00m etype, e_id \u001b[39min\u001b[39;49;00m edge_dict\u001b[39m.\u001b[39;49mitems() \u001b[39mif\u001b[39;49;00m etype \u001b[39min\u001b[39;49;00m G\u001b[39m.\u001b[39;49mcanonical_etypes}, cross_reducer \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     97\u001b[0m \u001b[39m# il controllo in etypes serve per non far rompere il codice quando non abbiamo tutti i tipi nel subgraph\u001b[39;00m\n\u001b[1;32m     99\u001b[0m new_h \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39msrc\u001b[39m\u001b[39m'\u001b[39m:{}, \u001b[39m'\u001b[39m\u001b[39mdst\u001b[39m\u001b[39m'\u001b[39m: {}}\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/heterograph.py:4909\u001b[0m, in \u001b[0;36mDGLGraph.multi_update_all\u001b[0;34m(self, etype_dict, cross_reducer, apply_node_func)\u001b[0m\n\u001b[1;32m   4907\u001b[0m     mfunc, rfunc, afunc \u001b[39m=\u001b[39m args\n\u001b[1;32m   4908\u001b[0m     g \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m etype \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m[etype]\n\u001b[0;32m-> 4909\u001b[0m     all_out[dtid]\u001b[39m.\u001b[39mappend(core\u001b[39m.\u001b[39;49mmessage_passing(g, mfunc, rfunc, afunc))\n\u001b[1;32m   4910\u001b[0m     merge_order[dtid]\u001b[39m.\u001b[39mappend(etid)  \u001b[39m# use edge type id as merge order hint\u001b[39;00m\n\u001b[1;32m   4911\u001b[0m \u001b[39mfor\u001b[39;00m dtid, frames \u001b[39min\u001b[39;00m all_out\u001b[39m.\u001b[39mitems():\n\u001b[1;32m   4912\u001b[0m     \u001b[39m# merge by cross_reducer\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/core.py:390\u001b[0m, in \u001b[0;36mmessage_passing\u001b[0;34m(g, mfunc, rfunc, afunc)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke message passing computation on the whole graph.\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \n\u001b[1;32m    367\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    381\u001b[0m \u001b[39m    Results from the message passing computation.\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m    384\u001b[0m     is_builtin(mfunc)\n\u001b[1;32m    385\u001b[0m     \u001b[39mand\u001b[39;00m is_builtin(rfunc)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    388\u001b[0m ):\n\u001b[1;32m    389\u001b[0m     \u001b[39m# invoke fused message passing\u001b[39;00m\n\u001b[0;32m--> 390\u001b[0m     ndata \u001b[39m=\u001b[39m invoke_gspmm(g, mfunc, rfunc)\n\u001b[1;32m    391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m     \u001b[39m# invoke message passing in two separate steps\u001b[39;00m\n\u001b[1;32m    393\u001b[0m     \u001b[39m# message phase\u001b[39;00m\n\u001b[1;32m    394\u001b[0m     \u001b[39mif\u001b[39;00m is_builtin(mfunc):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/core.py:351\u001b[0m, in \u001b[0;36minvoke_gspmm\u001b[0;34m(graph, mfunc, rfunc, srcdata, dstdata, edata)\u001b[0m\n\u001b[1;32m    349\u001b[0m         x \u001b[39m=\u001b[39m data_dict_to_list(graph, x, mfunc, lhs_target)\n\u001b[1;32m    350\u001b[0m         y \u001b[39m=\u001b[39m data_dict_to_list(graph, y, mfunc, rhs_target)\n\u001b[0;32m--> 351\u001b[0m     z \u001b[39m=\u001b[39m op(graph, x, y)\n\u001b[1;32m    352\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m     x \u001b[39m=\u001b[39m alldata[mfunc\u001b[39m.\u001b[39mtarget][mfunc\u001b[39m.\u001b[39min_field]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/ops/spmm.py:173\u001b[0m, in \u001b[0;36m_gen_spmm_func.<locals>.func\u001b[0;34m(g, x, y)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(g, x, y):\n\u001b[0;32m--> 173\u001b[0m     \u001b[39mreturn\u001b[39;00m gspmm(g, binary_op, reduce_op, x, y)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/ops/spmm.py:77\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(g, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m     75\u001b[0m         lhs_data, rhs_data \u001b[39m=\u001b[39m reshape_lhs_rhs(lhs_data, rhs_data)\n\u001b[1;32m     76\u001b[0m     \u001b[39m# With max and min reducers infinity will be returned for zero degree nodes\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m     ret \u001b[39m=\u001b[39m gspmm_internal(\n\u001b[1;32m     78\u001b[0m         g\u001b[39m.\u001b[39;49m_graph,\n\u001b[1;32m     79\u001b[0m         op,\n\u001b[1;32m     80\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39msum\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39mif\u001b[39;49;00m reduce_op \u001b[39m==\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mmean\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39melse\u001b[39;49;00m reduce_op,\n\u001b[1;32m     81\u001b[0m         lhs_data,\n\u001b[1;32m     82\u001b[0m         rhs_data,\n\u001b[1;32m     83\u001b[0m     )\n\u001b[1;32m     84\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     85\u001b[0m     \u001b[39m# lhs_data or rhs_data is None only in unary functions like ``copy-u`` or ``copy_e``\u001b[39;00m\n\u001b[1;32m     86\u001b[0m     lhs_data \u001b[39m=\u001b[39m (\n\u001b[1;32m     87\u001b[0m         [\u001b[39mNone\u001b[39;00m] \u001b[39m*\u001b[39m g\u001b[39m.\u001b[39m_graph\u001b[39m.\u001b[39mnumber_of_ntypes()\n\u001b[1;32m     88\u001b[0m         \u001b[39mif\u001b[39;00m lhs_data \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     89\u001b[0m         \u001b[39melse\u001b[39;00m lhs_data\n\u001b[1;32m     90\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:1032\u001b[0m, in \u001b[0;36mgspmm\u001b[0;34m(gidx, op, reduce_op, lhs_data, rhs_data)\u001b[0m\n\u001b[1;32m   1030\u001b[0m args \u001b[39m=\u001b[39m _cast_if_autocast_enabled(gidx, op, reduce_op, lhs_data, rhs_data)\n\u001b[1;32m   1031\u001b[0m \u001b[39mwith\u001b[39;00m _disable_autocast_if_enabled():\n\u001b[0;32m-> 1032\u001b[0m     \u001b[39mreturn\u001b[39;00m GSpMM\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[39m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[39m=\u001b[39m _functorch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mapply(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39msetup_context \u001b[39m==\u001b[39m _SingleLevelFunction\u001b[39m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mstaticmethod. For more details, please see \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/backend/pytorch/sparse.py:165\u001b[0m, in \u001b[0;36mGSpMM.forward\u001b[0;34m(ctx, gidx, op, reduce_op, X, Y)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(ctx, gidx, op, reduce_op, X, Y):\n\u001b[0;32m--> 165\u001b[0m     out, (argX, argY) \u001b[39m=\u001b[39m _gspmm(gidx, op, reduce_op, X, Y)\n\u001b[1;32m    166\u001b[0m     reduce_last \u001b[39m=\u001b[39m _need_reduce_last_dim(X, Y)\n\u001b[1;32m    167\u001b[0m     X_shape \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape \u001b[39mif\u001b[39;00m X \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/_sparse_ops.py:239\u001b[0m, in \u001b[0;36m_gspmm\u001b[0;34m(gidx, op, reduce_op, u, e)\u001b[0m\n\u001b[1;32m    237\u001b[0m arg_e_nd \u001b[39m=\u001b[39m to_dgl_nd_for_write(arg_e)\n\u001b[1;32m    238\u001b[0m \u001b[39mif\u001b[39;00m gidx\u001b[39m.\u001b[39mnumber_of_edges(\u001b[39m0\u001b[39m) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 239\u001b[0m     _CAPI_DGLKernelSpMM(\n\u001b[1;32m    240\u001b[0m         gidx,\n\u001b[1;32m    241\u001b[0m         op,\n\u001b[1;32m    242\u001b[0m         reduce_op,\n\u001b[1;32m    243\u001b[0m         to_dgl_nd(u \u001b[39mif\u001b[39;49;00m use_u \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    244\u001b[0m         to_dgl_nd(e \u001b[39mif\u001b[39;49;00m use_e \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    245\u001b[0m         to_dgl_nd_for_write(v),\n\u001b[1;32m    246\u001b[0m         arg_u_nd,\n\u001b[1;32m    247\u001b[0m         arg_e_nd,\n\u001b[1;32m    248\u001b[0m     )\n\u001b[1;32m    249\u001b[0m \u001b[39m# NOTE(zihao): actually we can avoid the following step, because arg_*_nd\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# refers to the data that stores arg_*. After we call _CAPI_DGLKernelSpMM,\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m# arg_* should have already been changed. But we found this doesn't work\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[39m# The workaround is proposed by Jinjing, and we still need to investigate\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[39m# where the problem is.\u001b[39;00m\n\u001b[1;32m    256\u001b[0m arg_u \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m arg_u \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m F\u001b[39m.\u001b[39mzerocopy_from_dgl_ndarray(arg_u_nd)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/dgl/_ffi/_ctypes/function.py:214\u001b[0m, in \u001b[0;36mFunctionBase.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    211\u001b[0m ret_val \u001b[39m=\u001b[39m DGLValue()\n\u001b[1;32m    212\u001b[0m ret_tcode \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m    213\u001b[0m check_call(\n\u001b[0;32m--> 214\u001b[0m     _LIB\u001b[39m.\u001b[39;49mDGLFuncCall(\n\u001b[1;32m    215\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[1;32m    216\u001b[0m         values,\n\u001b[1;32m    217\u001b[0m         tcodes,\n\u001b[1;32m    218\u001b[0m         ctypes\u001b[39m.\u001b[39;49mc_int(num_args),\n\u001b[1;32m    219\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(ret_val),\n\u001b[1;32m    220\u001b[0m         ctypes\u001b[39m.\u001b[39;49mbyref(ret_tcode),\n\u001b[1;32m    221\u001b[0m     )\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m _ \u001b[39m=\u001b[39m temp_args\n\u001b[1;32m    224\u001b[0m _ \u001b[39m=\u001b[39m args\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split(list_a, chunk_size):\n",
    "    for i in range(0, len(list_a), chunk_size):\n",
    "        yield list_a[i:i + chunk_size]\n",
    "\n",
    "batch_size = 64\n",
    "total_step_train = 0\n",
    "best_val_loss = 100\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    random.shuffle(train_pos_ids)\n",
    "    random.shuffle(train_neg_ids)\n",
    "    # training\n",
    "    losses = []\n",
    "    for batch in split(list(zip(train_pos_ids, train_neg_ids)), batch_size):\n",
    "        opt.zero_grad()\n",
    "        pos_ids, neg_ids = list(zip(*batch))\n",
    "        pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits = predict(g, model, pos_ids, neg_ids, train, sampler, batch_size)\n",
    "        pos_score = torch.bmm(pos_user_logits.view(batch_size, 1, model.n_out), pos_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        neg_score = torch.bmm(neg_user_logits.view(batch_size, 1, model.n_out), neg_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        loss = criterion(pos_score, neg_score, torch.ones(batch_size))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        loss_value = loss.item()\n",
    "        losses.append(loss_value)\n",
    "        writer.add_scalar(\"Loss/train/batch\", loss_value, total_step_train)\n",
    "        total_step_train += 1\n",
    "    writer.add_scalar(\"Loss/train\", sum(losses) / len(losses), epoch)\n",
    "    # validation\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for batch in split(list(zip(val_pos_ids, val_neg_ids)), batch_size):\n",
    "        pos_ids, neg_ids = list(zip(*batch))\n",
    "        pos_user_logits, pos_business_logits, neg_user_logits, neg_business_logits = predict(g, model, pos_ids, neg_ids, val, sampler, batch_size)\n",
    "        pos_score = torch.bmm(pos_user_logits.view(batch_size, 1, model.n_out), pos_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        neg_score = torch.bmm(neg_user_logits.view(batch_size, 1, model.n_out), neg_business_logits.view(batch_size, model.n_out, 1)).squeeze()\n",
    "        loss_value = criterion(pos_score, neg_score, torch.ones(batch_size)).item()\n",
    "        losses.append(loss_value)\n",
    "    val_loss = (sum(losses) / len(losses))\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': opt.state_dict(),\n",
    "        }, 'models/best_model.pt')\n",
    "\n",
    "    writer.add_scalar(\"Val/train\", val_loss, epoch)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pos_id = train['pos'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_business = train['pos'][1][train['pos'][0] == train['pos'][0][0]]\n",
    "neg_business = train['neg'][1][train['neg'][0] == train['pos'][0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([111108,  80217,  25553,  73239,  66461,  23450,  20934, 120272,  86879,\n",
       "         38967,  84152,  13612])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([120919])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_df = pd.read_csv('preprocessed/user_ids.csv')\n",
    "business_df = pd.read_csv('preprocessed/business_ids.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id:ID    U438yUH5aBVntI_CbVt8jg\n",
       "Name: 207300, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_df.loc[user_pos_id.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id:ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>111108</th>\n",
       "      <td>a0d09197752174e42a05ff2cf445fa91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80217</th>\n",
       "      <td>064a4a8a97aa17167a9427a19aca98ef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25553</th>\n",
       "      <td>33d2e8ccd5b8f4d14ad8e83b11444bc0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73239</th>\n",
       "      <td>3f2388115a0b7cc98b242191fdad7bf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66461</th>\n",
       "      <td>8e64483dbe1cb3d0662df91b83867345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23450</th>\n",
       "      <td>1fd668fc67cb62812e523ab153b411ce</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20934</th>\n",
       "      <td>f30c7b0034d553e0da7e07811841868b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120272</th>\n",
       "      <td>3d6954a8431403d9e6e8b293a943d6d2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86879</th>\n",
       "      <td>15aaec95654f4ba868dfb2547ec72193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38967</th>\n",
       "      <td>e1756e58d54f74ce2392ec5fe40d0eb5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84152</th>\n",
       "      <td>dd113aed595c4b7a50981891885def1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13612</th>\n",
       "      <td>05e514dcebb721d1b064977dc3c71489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          business_id:ID\n",
       "111108  a0d09197752174e42a05ff2cf445fa91\n",
       "80217   064a4a8a97aa17167a9427a19aca98ef\n",
       "25553   33d2e8ccd5b8f4d14ad8e83b11444bc0\n",
       "73239   3f2388115a0b7cc98b242191fdad7bf4\n",
       "66461   8e64483dbe1cb3d0662df91b83867345\n",
       "23450   1fd668fc67cb62812e523ab153b411ce\n",
       "20934   f30c7b0034d553e0da7e07811841868b\n",
       "120272  3d6954a8431403d9e6e8b293a943d6d2\n",
       "86879   15aaec95654f4ba868dfb2547ec72193\n",
       "38967   e1756e58d54f74ce2392ec5fe40d0eb5\n",
       "84152   dd113aed595c4b7a50981891885def1a\n",
       "13612   05e514dcebb721d1b064977dc3c71489"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.loc[pos_business.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id:ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120919</th>\n",
       "      <td>82ad544c4332ea410b5018b6b69b5a2d</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          business_id:ID\n",
       "120919  82ad544c4332ea410b5018b6b69b5a2d"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "business_df.loc[neg_business.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/filippominutella/miniconda3/lib/python3.10/site-packages/dgl/dataloading/dataloader.py:869: DGLWarning: Dataloader CPU affinity opt is not enabled, consider switching it on (see enable_cpu_affinity() or CPU best practices for DGL [https://docs.dgl.ai/tutorials/cpu/cpu_best_practises.html])\n",
      "  dgl_warning(f'Dataloader CPU affinity opt is not enabled, consider switching it on '\n",
      "/Users/filippominutella/miniconda3/lib/python3.10/site-packages/dgl/backend/pytorch/tensor.py:445: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  assert input.numel() == input.storage().size(), (\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "block_user = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "    g, {'user': [user_pos_id] }, sampler,\n",
    "    batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "pos_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "    g, {'business': pos_business }, sampler,\n",
    "    batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "neg_block_business = [blocks for _, _, blocks in dgl.dataloading.DataLoader(\n",
    "    g, {'business': neg_business }, sampler,\n",
    "    batch_size=batch_size, shuffle=False, drop_last=False, num_workers=1)][0]\n",
    "\n",
    "user = model(block_user, 'user')[0]\n",
    "p_business = model(pos_block_business, 'business')\n",
    "n_business = model(neg_block_business, 'business')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5728, 0.5628, 0.5803, 0.5380, 0.5070, 0.5362, 0.5807, 0.5817, 0.5664,\n",
       "        0.5480, 0.5818, 0.5637], grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(user.repeat(p_business.shape[0], 1).view(p_business.shape[0], 1, model.n_out), p_business.view(p_business.shape[0], model.n_out, 1)).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.5402, grad_fn=<SqueezeBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.bmm(user.repeat(n_business.shape[0], 1).view(n_business.shape[0], 1, model.n_out), n_business.view(n_business.shape[0], model.n_out, 1)).squeeze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
